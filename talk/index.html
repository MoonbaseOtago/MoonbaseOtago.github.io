<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/solarized.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>

					<h2>VROOM!</h2>
					<h4>A new high-end RISC-V implementation</h4>
					<p>Paul Campbell - October 2021</p>
					<p>paul@taniwha.com @moonbaseotago</p>
					<img width="300" src="assets/moonbase.png"><br>
					<small>
						<p>(C) Copyright Moonbase Otago 2021</p>
						<p>All rights reserved</p>
					</small>
				</section>
				<section>
					<h2>Executive summary</h2>
					<small>
					<ul>
						<li>Goal: Very high end RISC-V implementation – cloud server class</li>
						<li>Out of order, super scalar, speculative</li>
						<li>RV64-IMAFDCHB(V)</li>
						<li>Up to 8 IPC (instructions per clock) peak, goal ~4 average on ALU heavy work</li>
						<li>2-way simultaneous multithreading capable</li>
						<li>Multi-core</li>
						<li>Currently boots Linux on an AWS-FPGA instance</li>
						<li>Early (low) dhrystone numbers: ~2 DMips/MHz - still a work in progress. Goal ~5 with fully working BTC</li>
						<li>GPL3 – dual licensing possible</li>
					</ul>
					</small>
				</section>
				<section>
					<h2>Making something big and fast ...</h2>
					<small>
					<ul>
			
						<li>Our goal is &gt;4 IPC (average) with &gt;97% branch prediction and lots of cache, deep out-of-order pipelines and speculative execution for managing cache and branch miss latency </li>
						<li>General long term goal is a high end server class CPU, 5GHz+, multithreaded, 100+ instructions in flight at any one time</li>
					</ul>
					</small>
				</section>
				<section>
					<h2>Architectural Overview</h2>
					<img width="700" src="assets/overview.svg">
				</section>
				<section>
					<h2>1-3 Fetch and decoder</h2>
					<div class="r-hstack">
					<small>
					<ul>
						<li>4 32-bit 
						<br>instructions</li>
						<li>Or 8 16-bit 
						<br>instructions</li>
						<li>Or a mix</li>
						<li>Some instructions
						<br>swallowed (no-ops,
						<br>jumps)</li>
					</ul>
					</small>
					<img width="500" src="assets/decode.svg">
					</div>
				</section>
				<section>
					<h2>Branch Target cache</h2>
					<div align="left">
					<small>
					<ul>
					<li>Separate BTCs for each access mode, user mode flushed on MMU table switch</li>
					<li>32 entry call/return stack</li>
					<li>Combined global history/bimodal branch predictors</li>
					<li>Support for speculative branches/calls/returns</li>
					</ul>
					</small>
					</div>
				</section>
				<section>
					<h2>Instruction Bundles</h2>
					<div align="left">
					<small>
					<p>Decoded instructions are passed between stages in bundles containing:</p>
					<ul>
						<li>Functional unit type</li>
						<li>Command information (add/sub load/store etc)</li>
						<li>Source and dest registers (and renamed source registers)</li>
						<li>Immediate constant</li>
						<li>PC</li>
						<li>Branch target</li>
					</ul>
					<p>Eventually we'll do some instruction combining using this information (best place may be at entry to I$0 trace cache), or possibly at the rename stage</p> 
					</small>
					</div>
				</section>
				<section>

					<h2>Registers</h2>
					<div class="r-stack">
					<div class="r-hstack" valign="top">
						<img width="350" valign="top" src="assets/blank.png">
						<img width="400" valign="top" src="assets/registers.svg">
					</div>
					<small>
					<ul>
						<li>We use a combined 
							<br>register file</li>
						<li>Commit registers are
							<br>for instruction’s results
							<br>and are either 
							<br>eventually written to real 
							<br>registers or abandoned, one
							<br>commit register for every<br>commitQ entry</li>
						<li>Once a commitQ entry is committed it’s value is transferred to an
							architectural register </li>
						<li>Commit registers are shared between integer and FP regs</li>
					</ul>
					</small>
					</div>
				</section>
				<section>

					<h2>4 Renaming Stage</h2>
					<div class="r-stack">
					<div class="r-hstack" valign="top">
						<img width="350" valign="top" src="assets/blank.png">
						<img width="300" valign="top" src="assets/rename.svg">
					</div>
					<small>
					<ul>
						<li>Packs instruction
							<br>bundles</li>
						<li>Renames source
							<br>registers to pick up
							<br>speculative results
							<br>from commit registers,
							<br>scoreboard keeps track of
							<br>where the latest version
							<br>of each architectural register will be stored</li>
						<li>Keeps track of state when we do speculative misses</li>
					</ul>
					</small>
					</div>
				</section>
				<section>

					<h2>5+ Commit Queue</h2>
					<div class="r-stack">
					<div class="r-hstack" valign="top">
						<img width="300" valign="top" src="assets/blank.png">
						<img width="400" valign="top" src="assets/commit.svg">
					</div>
					<small>
					<ul>
						<li>Circular queue of pending 
							<br>instructions</li>
						<li>At some point
							<br>they are assigned
							<br>ALUs</li>
						<li>When near the end 
							<br>they are committed
							<br>(currently last 8 can be 
							<br>committed per clock)</li>
						<li>A resolved mispredicted branch or a trap can cause a partial or full commitQ flush</li>
					</ul>
					</small>
					</div>
				</section>
				<section>

					<h2>ALUs (functional units)</h2>
					<small>
					<ul>
						<li>3 arithmetic (add/sub/and/or/xor/etc) [currently 2]</li>
						<li>1 shift</li>
						<li>1 multiply/divide</li>
						<li>&gt;=1 FP	[a work in progress]</li>
						<li>1 or 2 branch [may be merged into arithmetic units]</li>
						<li>1 CSR/TRAP/privileged </li>
						<li>1 Load/Store (3 load/2 store per clock) [currently 2/1]</li>
						<li>Each commitQ entry is tagged for one of these</li>
					</ul>
					</small>
				</section>
				<section>

					<h2>ALUs (functional units) 2</h2>
					<div align = left>
					<small>
					<p>Inputs to a functional unit can be:</p>
					<ul>
						<li align="left">Result of 1 or two register reads                              </li>
						<li>An immediate constant</li>
						<li>PC of the instruction</li>
					</ul>
					<p>An instruction will not trigger execution until all its input registers are available.</p>
					</small>
					</div>

				</section>
				<section>

					<h2>Schedulers</h2>
					<small>
					<ul>
						<li>Each type of functional unit has a scheduler, they are independent</li>
						<li>It looks for instructions ready to execute (ie who’s source registers will have been calculated by the next clock)</li>
						<li>Schedules the N instructions ready to run closest to the commit end of the commitQ</li>
						<li>Load/Store scheduler wont re-order stores past stores, or loads past stores (but will reorder loads) – once scheduled (and virt-&gt;phys translation) further reordering can happen</li>
					</ul>
					</small>
				</section//>
				<section>

					<h2>6-7-8 schedulers</h2>
					<div class="r-hstack" valign="top">
					<small>
					<ul>
						<li>Basic ALU flow
							<br>looks like this</li>
						<li>Heavily pipelined
							<br>(input to reg write
							<br>can be bypassed 
							<br>to output of reg 
							<br>Basic ALU flow
							<br>read)</li>
					</ul>
					</small>
					<img width="200" valign="top" src="assets/alu.svg">
					</div>
				</section>
				<section>
					<h2>Load/Store/Fence Unit</h2>
					<small>
					<ul>
						<li>Single Unit</li>
						<li>Can handle 3 concurrent loads and 2 concurrent stores</li>
						<li>Loads can run in 1 clock if in cache (or snooped from storeQ) and in TLBs</li>
						<li>Also 1 clock speculatively if in storeQ</li>
						<li>Stores/Fences go in the storeQ, are executed in order, but only once their commitQ instructions are committed, they are abandoned if a speculative store is discarded</li>
						<li>Loads go in storeQ if they miss in cache, are fenced, or blocked by a pending access to the same cache line</li>
					</ul>
					</small>
				</section>
				<section>

					<h2>Load/Store Unit</h2>
					<img width="300" valign="top" src="assets/ls.svg">
				</section>
				<section>

					<h2>Virtual Memory</h2>
					<small>
					<div align="left">
					<ul>
						<li>Separate instruction and data 32 entry fully associative L1 TLBs</li>

						<li>Shared L2 TLB and table walker – 4 way associative 128+ entries</li>

						<li>Small cache of page data (to avoid upper page table refetches, takes part in cache coherency protocol)</li>

						<li>Table walker shares the instruction fetch port to the cache fabric (both are read only – I$1 cache coherency ports can have up to 8 concurrent transactions running at the same time)</li>	
						<li>16-bit unified (between HARTs) address space ID, or 15-bit unique (per HART) one</li>
					</ul>
					<p>Note: 'HART' is a RISC-V term that refers to a unit of execution - for example one portion of a multithreaded core</p>
					</div>
					</small>
				</section>
				<section>

					<h2>Branch Unit</h2>
					<small>
					<ul>
						<li>Conditional branches - compares 2 register values, if their relationship is not as predicted, forces a partial commitQ (after the branch instruction) flush and a new PC</li>
						<li>Subroutine calls – writes PC+2/4 to register as output</li>
						<li>Indirect branches (if not as predicted forces a partial commitQ flush and a new PC)</li>
						<li>(may yet merge this into the integer ALU allowing us to resolve multiple branches/clock)</li>
					</ul>
					</small>
				</section>
				<section>

					<h2>CSR Unit</h2>
					<small>
					<ul>
						<li>Creates most system CSRs (some are in other units FP/Vect)</li>
						<li>IRET, syscall instructions</li>
						<li>Interrupts and traps – equivalent to a branch with system state change</li>
						<li>load/stores that fail get converted into traps in the commitQ</li>
						<li>Interrupts and fetch/decode traps get forced into the instruction stream, instruction fetch then stops</li>
						<li>Always handled at last spot in commitQ – traps can flush subsequent instructions</li>
					</ul>
					</small>
				</section>
				<section>

					<h2>Performance</h2>
					<div align="left">
					<small>
					<p> Still a work in progress. Observed in the current implementation:</p>
					<ul>
						<li>Peak 8 instructions decoded per clock</li>
						<li>Peak 8 committed per clock</li>
						<li>5 clock branch misprediction penalty (often less or zero depending on what’s in the pipeline - mispredictions caught deep in the pipeline can be resolved at effectively 0 cost)</li>
					</ul>
					<p>Theoretical (one HART):</p>
					<ul>
						<li>Max 88 instructions in flight (104 if you count pending stores)</li>
						<li>(currently) 8 concurrent 512bit cache line fetches per L1 cache</li>
					</ul>
					</small>
					</div>
				</section>
				<section>
					<h2>Benchmarks</h2>
					<div align="left">
					<small>	
					<p>Up until now we've been running with the branch target cache broken on purpose so we can debug shootdown operations in the pipeline. Now (in the past few days) with linux booting we're starting to run dhrystone in linux user mode on the real hardware, and also in machine mode on the simulator. The global history part of the BTC is still broken, but the bimodal predictor and return stack seem to be working. The xilinx based hardware runs with a much smaller BTC - all numbers are at 25MHz</p>
					<table>
					<tr><td>Source</td><td>Dhrystone/sec</td><td>DMips</td><td>DMips/Mhz</td></tr>
					<tr><td>Hardware</td><td>81831</td><td>46.5</td><td>1.86</td></tr>
					<tr><td>Simulator</td><td>90093</td><td>51.2</td><td>2.05</td></tr>
					</table>
					<p>Looking at the architectural traces we could gain a small amount little here with a 3rd ALU, but the big performance issue is still the BTC hit rate - the simulator shows that its larger BTC is definitely a plus.</p>
					</small>
					</div>
				</section>
				<section>
					<h2>Putting it all together</h2>
					<img width="600" valign="top" src="assets/cpu1.svg">
				</section>
				<section>
					<h2>Multithreading</h2>
					<img width="600" valign="top" src="assets/cpu2.svg">
				</section>
				<section>
					<h2>A little further ...</h2>
					<p>This is what we have working today (without the L2)</p>
					<img width="600" valign="top" src="assets/sys1.svg">
				</section>
				<section>

					<h2>Die Size</h2>
					<img width="600" valign="top" src="assets/sys3.svg">
				</section>
				<section>
					<h2>Building Systems</h2>
					<img width="600" valign="top" src="assets/sys2.svg">
				</section>
				<section>
					<h2>Current System</h2>
					<small>
					<ul>
						<li>Written in Verilog, some parts auto-generated from C</li>
						<li>Currently being tested on an AWS FPGA instance - boots linux</li>
						<li>Xilinx VU9P Ultrascale (which is really 3 dies) </li>
						<li>Cut down to fit: 2 load 1 store units, 2 ALUs, 1 mult, 1 shifter, 32 entry commitQ, 8 entry storeQ, 32k I$1, 32k D$1, no L2, small BTC, max 56 instructions in flight</li>
						<li>Uses our cache coherency fabric, AWS’s DRAM controller</li>
						<li>Runs at 25MHz (largely for faster synthesis/routing times)</li>
						<li>Software support for serial and minimal hard drive, no networking yet</li>
					<ul>
					<img width="600" src="assets/chip.png">
					</small>
				</section>
				<section>
					<h2>AWS FPGA architecture</h2>
					<small>
					<img width="500" src="assets/vu9p.svg">
					<p  align="left">The CPU is instantiated in a VU9P FPGA along with AWS's support logic (top 1/3 of the RHS two dies on the previous page). 
					In our case we use the provided DRAM controller and PCIE interfaces to allow the host Linux CPU access to
					a small extra UART, a 'fake' disk controller, and a reset controller.</p>
					<p  align="left">On the host CPU a small user space program talks to registers in memory mapped
					PCIE space to provide a console (through the UART) and access to a 'disk' image 
					in a file. </p>
					</small>
				</section>
				<section>
					<h2>Where are we up to?</h2>
					<small>
					<ul>
						<li>Still very much a work in progress</li>
						<li>Most of the design is in place</li>
						<li>PLIC/CLIC/CLNT</li>
						<li>uart/faux disk/timers</li>
						<li>Coherent caching fabric</li>
						<li>Boots Linux on AWS FPGA instance</li>
						<li>Coded for multithreading (very not tested) and multiple CPUs (again not tested) – like cache/btc/queue sizes these are simple build options</li>
					</ul>
					</small>
				</section>
				<section>
					<h2>Eventual Goal</h2>
					<small>
					<p align="left">Much of the design is parameterized, we can change stuff easily, here’s a back of the envelope sketch of our goal:</p>	
					<ul>
						<li>1-5GHz (will likely involve adding ~2 pipe stages to the above description)</li>
						<li>2 HARTs/CPU (ie multithreaded)
						<li>CommitQ 64 entries - per HART</li>
						<li>StoreQ 32 entries - shared</li>
						<li>I$0 64+ entries of 8 instruction bundles - per HART</li>
						<li>I$1 64kbtytes - shared</li>
						<li>D$1 64kbytes - shared</li>
						<li>Combined L2 2-4Mb - shared</li>
						<li>This means ~192 max instructions in flight</li>
						<li>3 integer ALUs - shared</li>
						<li>1 shifter - shared</li>
						<li>3 load 2 store (per clock) load store unit - shared</li>
						<li>1 or 2 multipliers - shared</li>
						<li>1 or 2 FPUs - shared</li>
						<li>1 vector unit - shared</li>
						<li>PLIC/CLIC/CLNT - shared</li>
						<li>Bit/crypto extensions (mostly in shifter) - shared</li>
					</ul>
					</small>
				</section>
				<section>
					<h2>Meltdown/Spectre etc</h2>
					<div align="left">
					<small>
					<p>We’re not perfect (yet, still a work in progress), we do do the following mitigations:</p>
					<ul>
						<li>Separate BTCs between M/S/U operating modes</li>
						<li>BTC flushed on VM switch</li>
						<li>No speculative fetches to L1/2 caches until they pass VM access</li>
						<li>Fully associative TLB L1 with random replacement</li>
						<li>Wide D$1/I$1 way-ness (currently 32-way – also allows for large L1 caches with parallel TLB lookup) combined with random way replacement this muddies any signal an attacker is receiving</li>
						<li>Optional D$1 random replacement </li>
					</ul>
					</small>
					</div>
				</section>
				<section>
					<h2>Research</h2>
					<div align="left">
					<small>
					<p>One of the shorter term goals has been to get a system working well enough so that we can do benchmarks enabling us to optimize things like</p>
					<ul>
						<li>Cache sizes</li>
						<li>BTC size and architecture</li>
						<li>commitQ size</li>
						<li>storeQ size</li>
						<li>Test a multithreaded system</li>
						<li>Look at merging branch units and ALUs (this allows us to resolve multiple branches/clock)</li>
						<li>Investigate splitting out TLB lookup from the load/store unit into it’s own ALU – while it adds a clock it potentially allows us to do smart stuff with instruction reordering, it also may help reach Ghz speeds</li>
					</ul>
					<p>We’re at a point now though where the size of the AWS FPGA instances may limit what we can test at large scale</p>
					</small>
					</div>
				</section>
				<section>
					<h2>Next steps</h2>
					<small>
					<p align="left">Planned work:</p>
					<ul>
						<li>Finish FPU – about 50% done</li>
						<li>Work on BTC (currently deliberately broken to stress and debug pipeline commitQ shootdown)</li>
						<li>I$0 trace cache</li>
						<li>Expand LS unit to 3/2 from 2/1 load/store</li>
						<li>Rewrite cache coherency fabric with L2</li>
						<li>Spend some time on timing, we’ve purposely avoided spending too much time on low level timing – the current FPGA is big and slow, and nets that cross between dies kind of mess with any hope of representative timing – but it’s worth spending some time to hunt down particularly bad paths, we expect to repipe the final design by a couple of pipe stages to get to the Ghz range so some early warning would be useful</li>
						<li>B – bit manipulation – coded, not tested</li>
						<li>H - Virtualization – about 50% done</li>
						<li>V - Vector Unit (waiting for FP)</li>
						<li>Debug</li>
						<li>Crypto</li>
					</ul>
					</small>
				</section>
				<section>
					<h2>I$0 Trace cache</h2>
					<small>
					<p align="left">This is probably the most interesting enhancement we can do to the current system to up the issue rate in inner loops to a fixed 8 bundles/clock no matter what size the original instruction was</p>
					<ul>
						<li>Virtually tagged</li>
						<li>Contains instruction bundles recorded from the commit stage of the commitQ</li>
						<li>This is a great place to do instruction combining (timing wise)</li>
						<li>Bundles issue directly to the renamer saving a few clocks in the pipeline</li>
					</ul>
					</small>
				</section>
				<section>
					<h2>Licensing</h2>
					<p align="left">Once it’s usable by others:</p>
					<ul>
						<li>GPL 3</li>
						<li>Dual licensing available – looking for partners to actually build one</li>
					</ul>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				


				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
